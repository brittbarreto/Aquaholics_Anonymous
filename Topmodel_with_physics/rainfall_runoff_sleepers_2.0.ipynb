{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from osgeo import gdal\n",
    "from osgeo import osr\n",
    "from osgeo.gdalconst import *\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "driver.Register()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_from_storageRatio(targetStorageRatio, column):\n",
    "    #print(targetStorageRatio)\n",
    "    if targetStorageRatio < min(relationship_df[\"norm_SoilWatVol\"]):\n",
    "        return 0\n",
    "    upperStorageRatio = min(relationship_df[relationship_df[\"norm_SoilWatVol\"] > targetStorageRatio][\"norm_SoilWatVol\"])\n",
    "    lowerStorageRatio = max(relationship_df[relationship_df[\"norm_SoilWatVol\"] < targetStorageRatio][\"norm_SoilWatVol\"])\n",
    "    #print(upperStorageRatio, lowerStorageRatio)\n",
    "    upperRatio = relationship_df.loc[relationship_df[\"norm_SoilWatVol\"] == upperStorageRatio, column].iloc[0]\n",
    "    lowerRatio = relationship_df.loc[relationship_df[\"norm_SoilWatVol\"] == lowerStorageRatio, column].iloc[0]\n",
    "    if upperRatio == lowerRatio:\n",
    "        return upperRatio\n",
    "    \n",
    "    targetRatio = lowerRatio + (targetStorageRatio-lowerStorageRatio)*(upperRatio-lowerRatio)/(upperStorageRatio-lowerStorageRatio)\n",
    "    return targetRatio\n",
    "\n",
    "\n",
    "\n",
    "def plotFlow(times, day_month_series, precip, streamFlow, streamFlow_observed, m):    \n",
    "    fig,ax = plt.subplots(figsize=(14,5.5))\n",
    "    plt.gcf().text(0.65, 0.8, \"05-01 to 10-25-2017, hourly data\\ndecay m: {}\".format(m), \n",
    "                       fontsize=13, color='black')\n",
    "    ax.set_title(watershed_name + ' ', fontsize=20)\n",
    "    #ax.set_ylabel(r'$\\frac{A_{sat}}{A_{tot}}$', fontsize=22, labelpad=22, rotation='horizontal')\n",
    "    ax.set_ylabel(r'Streamflow per unit area [mm/hr]', fontsize=16, labelpad=16, rotation='vertical')\n",
    "    ax.set_xlabel(\"Time\", fontsize=16, labelpad=10, rotation='horizontal')\n",
    "    spaced_yticks = [min(streamFlow),\n",
    "                     min(streamFlow)+(max(streamFlow)-min(streamFlow))*0.25,\n",
    "                     min(streamFlow)+(max(streamFlow)-min(streamFlow))*0.5,\n",
    "                     min(streamFlow)+(max(streamFlow)-min(streamFlow))*0.75,\n",
    "                     max(streamFlow)]\n",
    "    ax.set_yticks(spaced_yticks)\n",
    "    ax.set_yticklabels(['{0:.4f}'.format(spaced_yticks[0]),\n",
    "                        '{0:.4f}'.format(spaced_yticks[1]),\n",
    "                        '{0:.4f}'.format(spaced_yticks[2]),\n",
    "                        '{0:.4f}'.format(spaced_yticks[3]),\n",
    "                        '{0:.4f}'.format(spaced_yticks[-1])], \n",
    "                       fontsize=13)\n",
    "    first_days_of_month_1 = [first for first in day_month_series if '01-' in first]\n",
    "    first_days_of_month = []\n",
    "    for first in first_days_of_month_1:\n",
    "        if first not in first_days_of_month:\n",
    "            first_days_of_month.append(first)\n",
    "    print(first_days_of_month)\n",
    "    first_days_of_month_indices = [31*24, 61*24, 92*24, 123*24, 153*24, 178*24, 208*64]\n",
    "    print(first_days_of_month_indices[:len(first_days_of_month)])\n",
    "    ax.set_xticks(first_days_of_month_indices[:len(first_days_of_month)])\n",
    "    ax.set_xticklabels(first_days_of_month)\n",
    "    \n",
    "    ax.plot(times, streamFlow_observed, color='k', linewidth=0.7)\n",
    "    ax.plot(times, streamFlow, color='r', linewidth=0.7)\n",
    "    \n",
    "    ax2 = ax.twinx()\n",
    "    #ax2.bar(times, precip, 0.2, color='dodgerblue')\n",
    "    #print(precip.head())\n",
    "    ax2.invert_yaxis()\n",
    "    #ax2.set_ylabel(r'Precipitation [mm]', fontsize=16, labelpad=16, rotation='vertical')\n",
    "    #y2_ticks = np.linspace(0, max(precip), max(precip))\n",
    "    #y2_ticklabels = [str(i) for i in y2_ticks]\n",
    "    #ax2.set_yticks(-1 * y2_ticks)\n",
    "    #ax2.set_yticklabels(y2_ticklabels)\n",
    "    fig.show()\n",
    "\n",
    "    \n",
    "    \n",
    "def plotDeficitRatio(times, day_month_series, precip, deficitRatio, m):\n",
    "    deficitRatio = deficitRatio*100\n",
    "    fig,ax = plt.subplots(figsize=(14, 5.5))\n",
    "    plt.gcf().text(0.65, 0.8, \"05-01 to 10-25-2017, hourly data\\ndecay m: {}\".format(m), \n",
    "                       fontsize=13, color='black')\n",
    "    ax.set_title(watershed_name + ' ', fontsize=20)\n",
    "    #ax.set_ylabel(r'$\\frac{A_{sat}}{A_{tot}}$', fontsize=22, labelpad=22, rotation='horizontal')\n",
    "    ax.set_ylabel(r'% water deficit', fontsize=16, labelpad=16, rotation='vertical')\n",
    "    ax.set_xlabel(\"Time\", fontsize=16, labelpad=10, rotation='horizontal')\n",
    "    spaced_yticks = [min(deficitRatio),\n",
    "                     min(deficitRatio)+(max(deficitRatio)-min(deficitRatio))*0.25,\n",
    "                     min(deficitRatio)+(max(deficitRatio)-min(deficitRatio))*0.5,\n",
    "                     min(deficitRatio)+(max(deficitRatio)-min(deficitRatio))*0.75,\n",
    "                     max(deficitRatio)]\n",
    "    ax.set_yticks(spaced_yticks)\n",
    "    ax.set_yticklabels(['{0:.2f}'.format(spaced_yticks[0]),\n",
    "                        '{0:.2f}'.format(spaced_yticks[1]),\n",
    "                        '{0:.2f}'.format(spaced_yticks[2]),\n",
    "                        '{0:.2f}'.format(spaced_yticks[3]),\n",
    "                        '{0:.2f}'.format(spaced_yticks[-1])], \n",
    "                       fontsize=13)\n",
    "    ax.plot(times, deficitRatio, color='r', linewidth=0.7)\n",
    "    \n",
    "    ax2 = ax.twinx()\n",
    "    ax2.bar(times, precip, 0.2, color='dodgerblue')\n",
    "    #print(precip.head())\n",
    "    ax2.invert_yaxis()\n",
    "    ax2.set_ylabel(r'Precipitation [mm]', fontsize=16, labelpad=16, rotation='vertical')\n",
    "    #y2_ticks = np.linspace(0, max(precip), max(precip)+1)\n",
    "    #y2_ticklabels = [str(i) for i in y2_ticks]\n",
    "    #ax2.set_yticks(-1 * y2_ticks)\n",
    "    #ax2.set_yticklabels(y2_ticklabels)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "    \n",
    "def plotAreaRatio(times, day_month_series, precip, areaRatio, m):\n",
    "    areaRatio = areaRatio*100\n",
    "    fig,ax = plt.subplots(figsize=(14,5.5))\n",
    "    plt.gcf().text(0.65, 0.8, \"05-01 to 10-25-2017, hourly data\\ndecay m: {}\".format(m), \n",
    "                       fontsize=13, color='black')\n",
    "    ax.set_title(watershed_name + ' ', fontsize=20)\n",
    "    #ax.set_ylabel(r'$\\frac{A_{sat}}{A_{tot}}$', fontsize=22, labelpad=22, rotation='horizontal')\n",
    "    ax.set_ylabel(r'% saturated area', fontsize=16, labelpad=16, rotation='vertical')\n",
    "    ax.set_xlabel(\"Time\", fontsize=16, labelpad=10, rotation='horizontal')\n",
    "    spaced_yticks = [min(areaRatio),\n",
    "                     min(areaRatio)+(max(areaRatio)-min(areaRatio))*0.25,\n",
    "                     min(areaRatio)+(max(areaRatio)-min(areaRatio))*0.5,\n",
    "                     min(areaRatio)+(max(areaRatio)-min(areaRatio))*0.75,\n",
    "                     max(areaRatio)]\n",
    "    ax.set_yticks(spaced_yticks)\n",
    "    ax.set_yticklabels(['{0:.2f}'.format(spaced_yticks[0]),\n",
    "                        '{0:.2f}'.format(spaced_yticks[1]),\n",
    "                        '{0:.2f}'.format(spaced_yticks[2]),\n",
    "                        '{0:.2f}'.format(spaced_yticks[3]),\n",
    "                        '{0:.2f}'.format(spaced_yticks[-1])], \n",
    "                       fontsize=13)\n",
    "    ax.plot(times, areaRatio, color='r', linewidth=0.7)\n",
    "    \n",
    "    ax2 = ax.twinx()\n",
    "    ax2.bar(times, precip, 0.2, color='dodgerblue')\n",
    "    #print(precip.head())\n",
    "    ax2.invert_yaxis()\n",
    "    ax2.set_ylabel(r'Precipitation [mm]', fontsize=16, labelpad=16, rotation='vertical')\n",
    "    #y2_ticks = np.linspace(0, max(precip), max(precip)+1)\n",
    "    #y2_ticklabels = [str(i) for i in y2_ticks]\n",
    "    #ax2.set_yticks(-1 * y2_ticks)\n",
    "    #ax2.set_yticklabels(y2_ticklabels)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.realpath('./')\n",
    "cwd = os.getcwd()\n",
    "\n",
    "watershed_name = \"Sleepers River\"\n",
    "watershed_foldername = \"sleepers\"\n",
    "watershed_nick = \"sleepers\"\n",
    "working_dir = (os.path.abspath(os.path.join(dir_path,'..')) + \"/data/\" + \n",
    "               watershed_foldername + os.path.sep)\n",
    "n_days = 20000\n",
    "run_number = 1\n",
    "decaying_transmissivity = True\n",
    "m = 0.02787428\n",
    "\n",
    "\n",
    "dem = working_dir + watershed_nick + \"_dem.tif\"\n",
    "elevations_ds = gdal.Open(dem, GA_ReadOnly)\n",
    "band = elevations_ds.GetRasterBand(1)\n",
    "cols, rows = elevations_ds.RasterXSize, elevations_ds.RasterYSize\n",
    "elevationValues = band.ReadAsArray(0, 0, cols, rows)\n",
    "geotransform = elevations_ds.GetGeoTransform()\n",
    "resol = np.round(geotransform[1])\n",
    "watershed_cells = np.where(elevationValues>0.0, 1, 0)\n",
    "A = np.sum(watershed_cells) * resol * resol\n",
    "\n",
    "#parameters for sand  \n",
    "Ks = 20  #micrometers per second\n",
    "Ks = Ks * 3600 / 1000000  #meters per hour\n",
    "psi_b = 0.1  #bubbling pressure / capillary head, for sand\n",
    "fi = 0.45  #porosity\n",
    "Sy = 0.22  #specific yield\n",
    "b = 0.0001354\n",
    "n = 15\n",
    "\n",
    "D = 2\n",
    "#ET = 3/24\n",
    "ETdict = {'Apr':0.0017/24, 'May':0.0017/24, 'Jun':0.0019/24, 'Jul':0.003/24,\n",
    "     'Aug':0.003/24, 'Sep':0.003/24, 'Oct':0.003/24}\n",
    "maxStorage = A * D * fi\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "run_foldername = \"{}_{}_{}\".format(watershed_nick, n_days, run_number) + os.path.sep\n",
    "relationship_table_filename = run_foldername+watershed_nick+\"_{}_{}_noDecay_ARatio_satFront_SoilWatVol.txt\".format(n_days, run_number)\n",
    "if decaying_transmissivity == True:\n",
    "    run_foldername = \"{}_{}_{}_decay{}\".format(watershed_nick, n_days, run_number, m) + os.path.sep\n",
    "    relationship_table_filename = run_foldername+watershed_nick+\"_{}_{}_decay{}_ARatio_satFront_SoilWatVol.txt\".format(n_days, run_number, m)\n",
    "relationship_df = pd.read_csv(relationship_table_filename, header=0, sep='\\t', index_col=0)\n",
    "relationship_df.head()\n",
    "\n",
    "precipitation_column_names = [\"timestep\", \"day\", \"hour\", \"precip\"]\n",
    "precipitation_filename = working_dir+watershed_nick+\"_2017_0401_1228_precipitation.csv\"\n",
    "precipitation_df = pd.read_csv(precipitation_filename, header=0, index_col=0, names=precipitation_column_names)\n",
    "date_time_obj = pd.to_datetime(precipitation_df.day)\n",
    "day_month = date_time_obj.apply(lambda x: x.strftime(\"%d-%b\"))\n",
    "precipitation_df[\"day_month\"] = day_month\n",
    "#precipitation_df = precipitation_df[precipitation_df[\"day\"] >= \"2017-05-01\"]\n",
    "#precipitation_df = precipitation_df[precipitation_df[\"day\"] <= \"10/25/2017\"]\n",
    "#print(precipitation_df[\"day\"].head())\n",
    "#print(precipitation_df.shape)\n",
    "\n",
    "observation_filename = working_dir+watershed_nick+\"_hourly_obs_2017_0401_1026.csv\"\n",
    "observation_column_names = [\"timestep\",\"day\",\"hour\",\"streamflow_cfs\",\"streamflow_ms\",\"ET_mhr\"]\n",
    "observation_df = pd.read_csv(observation_filename, header=0, index_col=0, \n",
    "                             usecols=[0,1,2,3,4,7], names=observation_column_names)\n",
    "date_time_obj = pd.to_datetime(observation_df.day)\n",
    "day_month = date_time_obj.apply(lambda x: x.strftime(\"%d-%b\"))\n",
    "observation_df[\"day_month\"] = day_month\n",
    "correct_date_format_Series = date_time_obj.apply(lambda x: x.strftime(\"%Y-%m-%d\"))\n",
    "observation_df[\"day\"] = correct_date_format_Series\n",
    "#observation_df = observation_df[observation_df[\"day\"] <= \"10/25/2017\"]#2017-10-25\"]\n",
    "#print(observation_df)\n",
    "#print(observation_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_Dict = {}\n",
    "initialStorageRatio = 0.95\n",
    "initialStorageVol = maxStorage * initialStorageRatio\n",
    "initialDeficitRatio = 1 - initialStorageRatio\n",
    "initialDeficitVol = maxStorage * initialDeficitRatio\n",
    "initialAreaRatio = interpolate_from_storageRatio(initialStorageRatio, \"AreaRatio\")\n",
    "print(\"initialAreaRatio is {}\".format(initialAreaRatio))\n",
    "initialAsat = initialAreaRatio * A\n",
    "#initialFront = interpolate_from_storageRatio(initialStorageRatio, \"Saturated_Front\")\n",
    "\n",
    "currentAsat = initialAsat\n",
    "currentStorageVol = initialStorageVol\n",
    "currentStorageRatio = initialStorageRatio\n",
    "print(\"initialStorageRatio is {}\".format(initialStorageRatio))\n",
    "#currentFront = initialFront\n",
    "\n",
    "timesteps = precipitation_df.index\n",
    "print(timesteps)\n",
    "initial_timestep = min(timesteps) + 29*24\n",
    "final_timestep = initial_timestep + (24*(1+31*3+30*2+25))\n",
    "#initial_timestep, final_timestep = min(timesteps), min(timesteps)+(24*(31+12))\n",
    "print(initial_timestep, final_timestep)\n",
    "range(min(timesteps),max(timesteps))\n",
    "\n",
    "observed_streamFlow_mmhr = observation_df[\"streamflow_ms\"][initial_timestep-1:final_timestep]*3600*1000  #mm/hr\n",
    "b_estimate = (observed_streamFlow_mmhr.iloc[0]/1000)/(initialStorageRatio)**n  #m/hr\n",
    "print(b_estimate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumP = 0\n",
    "sumET = 0\n",
    "sumQrunoff = 0\n",
    "sumQsubsurface = 0\n",
    "sumStreamFlow = 0\n",
    "sumStorageVol = initialStorageVol\n",
    "finalStorageVol = 0\n",
    "\n",
    "for timestep in range(initial_timestep, final_timestep+1):\n",
    "    print(\"Running timestep {}\".format(timestep))\n",
    "    previousAsat = currentAsat\n",
    "    previousStorageVol = currentStorageVol\n",
    "    previousStorageRatio = currentStorageRatio\n",
    "    #previousFront = currentFront\n",
    "    \n",
    "    date = precipitation_df['day'].loc[timestep]\n",
    "    hour = precipitation_df['hour'].loc[timestep]\n",
    "    day_month = precipitation_df['day_month'].loc[timestep]\n",
    "    P = precipitation_df['precip'].loc[timestep]/1000  #m/hr\n",
    "    totalP = P * A  #m^3/hr\n",
    "    sumP += totalP\n",
    "    Qrunoff = P * previousAsat  #m^3/hr\n",
    "    Qsubsurface = b_estimate * A * previousStorageRatio**n  #m^3/hr\n",
    "    #ET = observation_df['ET_mhr'].loc[timestep]*A  #m/hr\n",
    "    ET = ETdict[day_month[-3:]]*A\n",
    "    sumET += ET\n",
    "    deltaStorageVol = totalP - ET - Qrunoff - Qsubsurface\n",
    "    #sumStorageVol += deltaStorageVol\n",
    "    currentStorageVol = previousStorageVol + deltaStorageVol\n",
    "    if currentStorageVol < 0:\n",
    "        currentStorageVol = 0\n",
    "    if currentStorageVol > maxStorage:\n",
    "        Qrunoff += currentStorageVol - maxStorage\n",
    "        Qsubsurface = b_estimate * A * 1**n\n",
    "        currentStorageVol = maxStorage\n",
    "    norm_Qrunoff = Qrunoff / A\n",
    "    norm_Qsubsurface = Qsubsurface / A\n",
    "    norm_streamflow = norm_Qrunoff + norm_Qsubsurface\n",
    "    sumQrunoff += Qrunoff\n",
    "    sumQsubsurface += Qsubsurface\n",
    "    streamFlow = Qrunoff + Qsubsurface\n",
    "    sumStreamFlow += streamFlow\n",
    "    \n",
    "    currentStorageRatio = currentStorageVol/maxStorage\n",
    "    currentAreaRatio = interpolate_from_storageRatio(currentStorageRatio, \"AreaRatio\")\n",
    "    currentAsat = currentAreaRatio * A\n",
    "    #currentFront = interpolate_from_storageRatio(currentStorageRatio, \"Saturated_Front\")\n",
    "    output_Dict[timestep] = [date, hour, day_month,\n",
    "                             P*1000, Qrunoff, Qsubsurface,\n",
    "                             currentStorageRatio, currentAreaRatio,\n",
    "                             streamFlow, norm_streamflow*1000]\n",
    "    finalStorageVol = currentStorageVol\n",
    "\n",
    "print()\n",
    "print(sumP, sumET, sumQrunoff, sumQsubsurface, sumStreamFlow, sumStorageVol)\n",
    "print(\"Final water balance is {} m^3\".format(sumP - sumET - sumStreamFlow))\n",
    "print(\"{} - {} = {}\".format(finalStorageVol, initialStorageVol, finalStorageVol-initialStorageVol))\n",
    "\n",
    "output_df = pd.DataFrame.from_dict(output_Dict,\n",
    "                                   orient = 'index',\n",
    "                                   columns = [\"date\", \"hour\", \"day_month\",\n",
    "                                              \"P\", \"Qse\",\"Qgw\", \n",
    "                                              \"storageRatio\", \"areaRatio\",\n",
    "                                              \"model_m3hr\", \"model_mmhr\"])\n",
    "\n",
    "output_df[\"observ_mmhr\"] = observed_streamFlow_mmhr\n",
    "\n",
    "model_output_name = \"{}{}_{}_{}_noDecay_modelOutput.csv\".format(run_foldername, watershed_nick, \n",
    "                                                                               n_days, run_number)\n",
    "if decaying_transmissivity == True:\n",
    "    model_output_name = \"{}{}_{}_{}_decay{}_modelOutput.csv\".format(run_foldername, watershed_nick, \n",
    "                                                                             n_days, run_number, m)\n",
    "print(model_output_name)    \n",
    "output_df.to_csv(model_output_name, sep=',')\n",
    "\n",
    "timesteps = output_df.index\n",
    "day_month_series = output_df[\"day_month\"]\n",
    "hour_series = output_df[\"hour\"]\n",
    "date_series = output_df[\"date\"]\n",
    "precip = output_df[\"P\"]\n",
    "Qse = output_df[\"Qse\"]\n",
    "Qgw = output_df[\"Qgw\"]\n",
    "streamFlow = output_df[\"model_m3hr\"]\n",
    "norm_streamFlow = output_df[\"model_mmhr\"]\n",
    "storageRatio = output_df[\"storageRatio\"]\n",
    "areaRatio = output_df[\"areaRatio\"]\n",
    "#print(observed_streamFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(observed_streamFlow_mmhr.head())\n",
    "plotFlow(timesteps, day_month_series, precip, norm_streamFlow, observed_streamFlow_mmhr, m)\n",
    "plotDeficitRatio(timesteps, day_month_series, precip, 1-storageRatio, m)\n",
    "plotAreaRatio(timesteps, day_month_series, precip, areaRatio, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_estimate = (observed_streamFlow.iloc[1]/1000)/(initialStorageRatio)**n\n",
    "print(observed_streamFlow.iloc[1])\n",
    "print(btry/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig,ax = plt.subplots(figsize=(13, 5.5))\n",
    "x_ = np.log(3600*output_df.norm_streamFlow)\n",
    "y_ = np.log(3600*output_df.observed_mh)\n",
    "#print(y_)\n",
    "plt.gcf().text(0.65, 0.8, \"05-01 to 10-25-2017, hourly data\\ndecay m: {}\".format(m), \n",
    "                   fontsize=13, color='black')\n",
    "ax.set_title(watershed_name + ' ', fontsize=20)\n",
    "#ax.set_ylabel(r'$\\frac{A_{sat}}{A_{tot}}$', fontsize=22, labelpad=22, rotation='horizontal')\n",
    "ax.set_ylabel(r'Observed streamflow [mm/hr]', fontsize=16, labelpad=16, rotation='vertical')\n",
    "ax.set_xlabel(\"Modeled streamflow [mm/hr]\", fontsize=16, labelpad=10, rotation='horizontal')\n",
    "#spaced_yticks = [min(y_),\n",
    "#                 min(y_)+(max(y_)-min(y_))*0.25,\n",
    "#                 min(y_)+(max(y_)-min(y_))*0.5,\n",
    "#                 min(y_)+(max(y_)-min(y_))*0.75,\n",
    "#                 max(y_)]\n",
    "#ax.set_yticks(spaced_yticks)\n",
    "#ax.set_yticklabels(['{0:.4f}'.format(spaced_yticks[4]),\n",
    "#                    '{0:.4f}'.format(spaced_yticks[3]),\n",
    "#                    '{0:.4f}'.format(spaced_yticks[2]),\n",
    "#                    '{0:.4f}'.format(spaced_yticks[1]),\n",
    "#                    '{0:.4f}'.format(spaced_yticks[0])], \n",
    "#                   fontsize=13)\n",
    "#ax.set_xticklabels([0, 0.002, 0.004, 0.006, 0.008, 0.10], fontsize=13)\n",
    "plt.scatter(3600*output_df.norm_streamFlow, 3600*output_df.observed_mh, s=2, c='k', alpha=0.8)\n",
    "#plt.axis([0, max(x_), 0, max(y_)])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script finds the start and end points of spikes in streamflow time-series for a single hydrogrpah.\n",
    "The start and end points are used to create a new data frame of data that only includes the storm periods.\n",
    "Acknowledgement:\n",
    "This code is adapted from https://github.com/UVAdMIST/Norfolk_Groundwater_Model/blob/master/Preprocess/Storm_train_data.py\n",
    "which is originally used for GWL data.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from scipy.signal import argrelmax, find_peaks\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "\n",
    "n_lag = 0  # 58\n",
    "n_ahead = 0  # 18\n",
    "\n",
    "# load dataset\n",
    "\n",
    "#data_path = r\"C:\\Users\\feder\\Documents\\CUAHSI\\resources\\iman\\02450250.csv\"\n",
    "observ_flow = np.asarray(3600*output_df.observ_mh, dtype='float64')\n",
    "#dataset = pd.read_csv(data_path, header=0, sep=',', index_col=None, parse_dates=True,\n",
    "#                      infer_datetime_format=True)\n",
    "#model_flow = np.asarray(dataset[\"Discharge\"], dtype='float64')\n",
    "\n",
    "\n",
    "# find peaks\n",
    "# max_idx = argrelmax(second_dev_smoothed)[0]\n",
    "#scipy.signal.find_peaks(x, height=None, threshold=None, distance=2, \n",
    "#                        prominence=2, width=None, wlen=None, rel_height=0.5, \n",
    "#                        plateau_size=None)[source]¶\n",
    "found_peaks = find_peaks(observ_flow, \n",
    "                         #prominence=2, \n",
    "                         distance=2,\n",
    "                         height=0.4,\n",
    "                         width=3)\n",
    "#print(found_peaks)\n",
    "peak_times = found_peaks[0]+721\n",
    "peak_start_times = np.floor(found_peaks[1]['left_ips'])+721\n",
    "peak_end_times = np.ceil(found_peaks[1]['right_ips'])+721\n",
    "print(peak_times, peak_start_times, peak_end_times)\n",
    "print(output_df.loc[output_df.index.isin(peak_times)]['day_month'])\n",
    "output_df[\"is_peak\"] = \"NO\"\n",
    "output_df.loc[output_df.index.isin(peak_times),'is_peak'] = \"YES\"\n",
    "output_df.loc[output_df.index.isin(peak_times),'peak_start'] = peak_start_times\n",
    "output_df.loc[output_df.index.isin(peak_times),'peak_end'] = peak_end_times\n",
    "#print(output_df[0:4969])\n",
    "#print(output_df.loc[721:4966,'day_month'])\n",
    "output_df[\"mw_start\"] = output_df.index-3\n",
    "output_df[\"mw_end\"] = output_df.index+3\n",
    "output_df.to_csv(model_output_name, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use first and second derivative\n",
    "fig = plt.figure(figsize=(8, 14))\n",
    "gs = gridspec.GridSpec(7, 1)\n",
    "\n",
    "ax0 = plt.subplot(gs[0])\n",
    "ax0.set_title('GWL')\n",
    "ax0.plot(observ_flow)\n",
    "\n",
    "first_dev = np.gradient(observ_flow)\n",
    "ax1 = plt.subplot(gs[1])\n",
    "ax1.set_title('1st derivative')\n",
    "ax1.plot(first_dev)\n",
    "\n",
    "second_dev = np.gradient(first_dev)\n",
    "ax2 = plt.subplot(gs[2])\n",
    "ax2.set_title('2nd derivative')\n",
    "#ax2.plot(second_dev)\n",
    "\n",
    "first_dev_clipped = np.clip(np.abs(np.gradient(first_dev)), 0.0001, 2)\n",
    "ax3 = plt.subplot(gs[3])\n",
    "ax3.set_title('first derivative absolute value + clipping')\n",
    "#ax3.plot(first_dev_clipped)\n",
    "\n",
    "second_dev_clipped = np.clip(np.abs(np.gradient(second_dev)), 0.0001, 2)\n",
    "ax4 = plt.subplot(gs[4])\n",
    "ax4.set_title('second derivative absolute value + clipping')\n",
    "#ax4.plot(second_dev_clipped)\n",
    "\n",
    "first_dev_smoothed = gaussian_filter1d(first_dev, 5)\n",
    "ax5 = plt.subplot(gs[5])\n",
    "ax5.set_title('Smoothing applied to 1st derivative')\n",
    "#ax5.plot(first_dev_smoothed)\n",
    "\n",
    "second_dev_smoothed = gaussian_filter1d(second_dev_clipped, 5)\n",
    "ax6 = plt.subplot(gs[6])\n",
    "ax6.set_title('Smoothing applied to second derivative')\n",
    "#ax6.plot(second_dev_smoothed)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# max_1stdev = argrelmax(first_dev_smoothed)[0]\n",
    "# print(max_idx, max_1stdev)\n",
    "# find indices where first derivative == 0\n",
    "first_dev_zeros = np.where(first_dev == 0)\n",
    "first_dev_zeros = first_dev_zeros[0]\n",
    "first_dev_zeros = np.insert(first_dev_zeros, 0, 33)  # MMPS-175, add 33 to beginning because there was no start\n",
    "\n",
    "# find location of max first derivative\n",
    "found_first_dev = find_peaks(first_dev_smoothed, height=2)\n",
    "# found_second_dev = find_peaks(second_dev_smoothed, height=0.00015, distance=100, prominence=0.0002)\n",
    "# found_second_dev = np.asarray(found_second_dev[0])\n",
    "\n",
    "# find indices of zero values that bracket max value\n",
    "start_list = []\n",
    "end_list = []\n",
    "for i in found_first_dev[0]:\n",
    "    min_list = []\n",
    "    max_list = []\n",
    "    for j in first_dev_zeros:\n",
    "        if i < j:\n",
    "            min_list.append(j)\n",
    "            # print(\"appended\", j, \"to min_list\")\n",
    "        if i > j:\n",
    "            max_list.append(j)\n",
    "            # print(\"appended\", j, \"to max_list\")\n",
    "        else:\n",
    "            continue\n",
    "    end_list.append(min(min_list))\n",
    "    start_list.append(max(max_list))\n",
    "    # end_list.append(max(max_list)-5)\n",
    "\n",
    "# create pairs of start and peak values\n",
    "potential_start = []\n",
    "potential_peak = []\n",
    "for i in range(0, len(start_list), 1):\n",
    "    for j in range(0, len(found_peaks[0]), 1):\n",
    "        if i < len(start_list)-1:\n",
    "            if start_list[i] < found_peaks[0][j]:\n",
    "                if found_peaks[0][j] < start_list[i+1]:\n",
    "                    # print(start_list[i], found_peaks[0][j])\n",
    "                    potential_start.append(start_list[i])\n",
    "                    potential_peak.append(found_peaks[0][j])\n",
    "        if i == len(start_list)-1:\n",
    "            if start_list[i] < found_peaks[0][j]:\n",
    "                # print(start_list[i], found_peaks[0][j])\n",
    "                potential_start.append(start_list[i])\n",
    "                potential_peak.append(found_peaks[0][j])\n",
    "\n",
    "# filter pairs of values to remove ones that have the same start\n",
    "filtered_start = []\n",
    "filtered_peak = []\n",
    "for i in range(0, len(potential_start), 1):  # potential start and peak list have same length\n",
    "    if potential_start[i] != potential_start[i-1]:\n",
    "        # print(potential_start[i], potential_peak[i])\n",
    "        filtered_start.append(potential_start[i])\n",
    "        filtered_peak.append(potential_peak[i])\n",
    "\n",
    "# add lag data plus forecast to start and forecast data plus 24hrs to end\n",
    "final_start = []\n",
    "final_peak = []\n",
    "for i, j in zip(filtered_start, filtered_peak):\n",
    "    new_start = i - (n_lag + n_ahead)\n",
    "    new_peak = j + n_ahead\n",
    "    # print(new_start, new_peak)\n",
    "    final_start.append(new_start)\n",
    "    final_peak.append(new_peak)\n",
    "\n",
    "# save dates of start and end lists\n",
    "df_list = []\n",
    "df_len_list = []\n",
    "for i, j in zip(final_start, final_peak):\n",
    "    print(i, j)\n",
    "    df = dataset.iloc[i:j+1]\n",
    "    df_len = len(df)\n",
    "    df_list.append(df)\n",
    "    df_len_list.append(df_len)\n",
    "storms_df = pd.concat(df_list).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# get average length of storms\n",
    "storm_avg = round(sum(df_len_list)/len(df_len_list))\n",
    "print(\"Average len of storms\", \"is: \", storm_avg)\n",
    "\n",
    "# shuffle dfs in df_list to create 1000 bootstrap replicates\n",
    "count = 0\n",
    "while count <= 1000:\n",
    "    if count == 0:\n",
    "        bs_df = pd.concat(df_list).drop_duplicates().reset_index(drop=True)\n",
    "        bs_df.to_csv(bs_path + \"bs0.csv\", index=False)\n",
    "    if count >= 1:\n",
    "        if count % 25 == 0:\n",
    "            print(count)\n",
    "        df_list2 = df_list\n",
    "        bs_df_list = random.choices(df_list2, k=len(df_list))  # this samples df_list with replacement\n",
    "        bs_df = pd.concat(bs_df_list).reset_index(drop=True)\n",
    "        bs_df.to_csv(bs_path + \"bs\" + str(count) + \".csv\", index=False)\n",
    "    count += 1\n",
    "\n",
    "# plot observed gwl with start, end, and max f' points\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel('Time Index')\n",
    "ax.set_ylabel('Streamflow (cfs)')\n",
    "ax.plot(model_flow)\n",
    "# ax.scatter(found_first_dev[0], gwl_test[found_first_dev[0]], marker='o', color='blue', label=\"Max f'\")\n",
    "ax.scatter(final_start, model_flow[final_start], marker='x', color='red', label='Start')\n",
    "ax.scatter(final_peak, model_flow[final_peak], marker='P', color='k', label='Peak')\n",
    "# ax.scatter(start_list, gwl_test[start_list], marker='x', color='red', label='Start')\n",
    "# ax.scatter(end_list, gwl_test[end_list], marker='^', color='green', label='End')\n",
    "# ax.scatter(found_peaks[0], gwl_test[found_peaks[0]], marker='P', color='k', label='Peak')\n",
    "# ax.scatter(first_dev_zeros, gwl_test[first_dev_zeros], marker='*', color='purple', label='first_dev_zeros')\n",
    "# ax.scatter(max_idx, gwl_test[max_idx], marker='p', color='orange', label='max_idx')\n",
    "# ax.scatter(found_peaks[0], gwl_test[found_peaks[0]], marker='P', color='k', label='found_peaks')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "data_path = \"C:/Users/feder/Documents/CUAHSI/repos/Aquaholics_Anonymous/Topmodel_with_physics/sleepers_20000_1_decay0.05\"\n",
    "storms_df.to_csv(data_path + \"_no_blanks_SI_storms.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
